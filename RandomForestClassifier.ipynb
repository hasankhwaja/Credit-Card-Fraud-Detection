{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0178a3d-ca8b-4b40-ab24-a8dd50511113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import radians, cos, sin, asin, sqrt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from scipy.stats import randint \n",
    "import warnings\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e1e76c-64d9-49a7-8833-0216d2f9ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fraud_data(file_path):\n",
    "    \"\"\"\n",
    "    Preprocess the fraud dataset and split into training and validation sets.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Processed training and validation sets (X_train, X_val, y_train, y_val) and the fitted scaler.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert 'trans_date_trans_time' to datetime\n",
    "    df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "\n",
    "    # Calculate age using 'dob' and transaction year\n",
    "    df['transaction_year'] = df['trans_date_trans_time'].dt.year\n",
    "    df['year_of_birth'] = pd.to_datetime(df['dob']).dt.year\n",
    "    df['age'] = df['transaction_year'] - df['year_of_birth']\n",
    "    df.drop(columns=['dob', 'transaction_year', 'year_of_birth'], inplace=True)\n",
    "\n",
    "    # Drop irrelevant columns\n",
    "    irrelevant_columns = ['Unnamed: 0', 'cc_num', 'trans_num', 'street']\n",
    "    df_cleaned = df.drop(columns=irrelevant_columns)\n",
    "\n",
    "    # Haversine function to calculate distance\n",
    "    def haversine(lat1, lon1, lat2, lon2):\n",
    "        lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "        c = 2 * asin(sqrt(a))\n",
    "        r = 6371  # Radius of Earth in kilometers.\n",
    "        return c * r\n",
    "\n",
    "    # Calculate distance and add to the dataset\n",
    "    df_cleaned['distance'] = df_cleaned.apply(\n",
    "        lambda row: haversine(row['lat'], row['long'], row['merch_lat'], row['merch_long']), axis=1)\n",
    "\n",
    "    # Create bins for latitude and longitude\n",
    "    n_bins = 10\n",
    "    df_cleaned['lat_bucket'] = pd.cut(df_cleaned['lat'], bins=n_bins, labels=False)\n",
    "    df_cleaned['long_bucket'] = pd.cut(df_cleaned['long'], bins=n_bins, labels=False)\n",
    "    df_cleaned['merch_lat_bucket'] = pd.cut(df_cleaned['merch_lat'], bins=n_bins, labels=False)\n",
    "    df_cleaned['merch_long_bucket'] = pd.cut(df_cleaned['merch_long'], bins=n_bins, labels=False)\n",
    "\n",
    "    # Encode categorical columns\n",
    "    categorical_columns = ['merchant', 'category', 'gender', 'job']\n",
    "    label_encoders = {}\n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        df_cleaned[col] = le.fit_transform(df_cleaned[col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "    # Drop columns that are no longer needed\n",
    "    columns_to_drop = ['trans_date_trans_time', 'first', 'last', 'city', 'state', 'zip', 'lat', 'long', 'merch_lat',\n",
    "                         'merch_long']\n",
    "    df_cleaned = df_cleaned.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Separate features and target variable\n",
    "    X = df_cleaned.drop(columns=['is_fraud'])\n",
    "    y = df_cleaned['is_fraud']\n",
    "\n",
    "    # Normalize numerical columns\n",
    "    numerical_columns = ['amt', 'age', 'distance', 'lat_bucket', 'long_bucket', 'merch_lat_bucket', 'merch_long_bucket']\n",
    "    scaler = StandardScaler()\n",
    "    X[numerical_columns] = scaler.fit_transform(X[numerical_columns])\n",
    "\n",
    "    # Split into training and validation sets (fixed parameters)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_val, y_train, y_val, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e046e558-0dd7-4d28-9f85-63d67d214572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fraud_test(file_path, scaler):\n",
    "    \"\"\"\n",
    "    Preprocess the fraud dataset for testing.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "        scaler (StandardScaler): A scaler fitted on the training data.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Processed features and target (X, y).\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert 'trans_date_trans_time' to datetime\n",
    "    df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "\n",
    "    # Calculate age using 'dob' and transaction year\n",
    "    df['transaction_year'] = df['trans_date_trans_time'].dt.year\n",
    "    df['year_of_birth'] = pd.to_datetime(df['dob']).dt.year\n",
    "    df['age'] = df['transaction_year'] - df['year_of_birth']\n",
    "    df.drop(columns=['dob', 'transaction_year', 'year_of_birth'], inplace=True)\n",
    "\n",
    "    # Drop irrelevant columns\n",
    "    irrelevant_columns = ['Unnamed: 0', 'cc_num', 'trans_num', 'street']\n",
    "    df_cleaned = df.drop(columns=irrelevant_columns)\n",
    "\n",
    "    # Haversine function to calculate distance\n",
    "    def haversine(lat1, lon1, lat2, lon2):\n",
    "        lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "        c = 2 * asin(sqrt(a))\n",
    "        r = 6371  # Radius of Earth in kilometers.\n",
    "        return c * r\n",
    "\n",
    "    # Calculate distance and add to the dataset\n",
    "    df_cleaned['distance'] = df_cleaned.apply(\n",
    "        lambda row: haversine(row['lat'], row['long'], row['merch_lat'], row['merch_long']), axis=1)\n",
    "\n",
    "    # Create bins for latitude and longitude\n",
    "    n_bins = 10\n",
    "    df_cleaned['lat_bucket'] = pd.cut(df_cleaned['lat'], bins=n_bins, labels=False)\n",
    "    df_cleaned['long_bucket'] = pd.cut(df_cleaned['long'], bins=n_bins, labels=False)\n",
    "    df_cleaned['merch_lat_bucket'] = pd.cut(df_cleaned['merch_lat'], bins=n_bins, labels=False)\n",
    "    df_cleaned['merch_long_bucket'] = pd.cut(df_cleaned['merch_long'], bins=n_bins, labels=False)\n",
    "\n",
    "    # Encode categorical columns\n",
    "    categorical_columns = ['merchant', 'category', 'gender', 'job']\n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        df_cleaned[col] = le.fit_transform(df_cleaned[col])\n",
    "\n",
    "    # Drop columns that are no longer needed\n",
    "    columns_to_drop = ['trans_date_trans_time', 'first', 'last', 'city', 'state', 'zip', 'lat', 'long', 'merch_lat', 'merch_long']\n",
    "    df_cleaned = df_cleaned.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Separate features and target variable\n",
    "    X = df_cleaned.drop(columns=['is_fraud'])\n",
    "    y = df_cleaned['is_fraud']\n",
    "\n",
    "    # Normalize numerical columns using the scaler from training\n",
    "    numerical_columns = ['amt', 'age', 'distance', 'lat_bucket', 'long_bucket', 'merch_lat_bucket', 'merch_long_bucket']\n",
    "    X[numerical_columns] = scaler.transform(X[numerical_columns])\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbe7fd3d-49e3-4932-b01d-3775fa909a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:\n",
      "Accuracy: 0.9975899897815567\n",
      "Precision: 0.8977777777777778\n",
      "Recall: 0.6644736842105263\n",
      "F1 Score: 0.7637051039697542\n"
     ]
    }
   ],
   "source": [
    "# Preprocess training data\n",
    "X_train, X_val, y_train, y_val, scaler = preprocess_fraud_data('fraudTrain.csv')\n",
    "\n",
    "# Train a Random Forest classifier on the training set\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_pred_val = rf.predict(X_val)\n",
    "print(\"Validation Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred_val))\n",
    "print(\"Precision:\", precision_score(y_val, y_pred_val))\n",
    "print(\"Recall:\", recall_score(y_val, y_pred_val))\n",
    "print(\"F1 Score:\", f1_score(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ebda691-16a0-418f-bb72-69f37b222d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      " amt                  0.415260\n",
      "category             0.169889\n",
      "unix_time            0.078189\n",
      "distance             0.061202\n",
      "city_pop             0.056923\n",
      "age                  0.056854\n",
      "merchant             0.051758\n",
      "job                  0.043802\n",
      "merch_long_bucket    0.013958\n",
      "lat_bucket           0.013557\n",
      "merch_lat_bucket     0.013322\n",
      "long_bucket          0.012946\n",
      "gender               0.012339\n",
      "dtype: float64\n",
      "Selected features based on importance: ['merchant', 'category', 'amt', 'city_pop', 'job', 'unix_time', 'age', 'distance']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hasan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\hasan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Display feature importances\n",
    "feature_scores = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "print(\"Feature Importances:\\n\", feature_scores)\n",
    "\n",
    "# Select features based on importance (threshold = 0.02)\n",
    "selector = SelectFromModel(rf, threshold=0.02, prefit=True)\n",
    "X_train_sel = selector.transform(X_train)\n",
    "X_val_sel = selector.transform(X_val)\n",
    "selected_features = X_train.columns[selector.get_support()]\n",
    "print(\"Selected features based on importance:\", list(selected_features))\n",
    "\n",
    "# Ignore warning about feature names\n",
    "warnings.filterwarnings(\"ignore\", message=\"X has feature names, but SelectFromModel was fitted without feature names\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8edb84f4-8df1-48a3-8009-91f1ca147782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'bootstrap': True, 'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 3, 'n_estimators': 71}\n",
      "Best cross-validation F1 score: 0.6985929718691194\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning using RandomizedSearchCV on selected features\n",
    "param_dist = { \n",
    "    'n_estimators': randint(50, 150), \n",
    "    'max_depth': [None] + list(range(5, 15)), \n",
    "    'min_samples_split': randint(2, 6), \n",
    "    'min_samples_leaf': randint(1, 6), \n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "rf_tuned = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_tuned, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=10, \n",
    "    cv=2, \n",
    "    scoring='f1', \n",
    "    random_state=42, \n",
    "    n_jobs=-1\n",
    ")\n",
    "random_search.fit(X_train_sel, y_train)\n",
    "print(\"Best parameters found:\", random_search.best_params_)\n",
    "print(\"Best cross-validation F1 score:\", random_search.best_score_)\n",
    "best_rf = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b481e34e-3ebd-4aa4-8f96-e04f60fb827e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics:\n",
      "Accuracy: 0.9976624876961198\n",
      "Precision: 0.7694267515923567\n",
      "Recall: 0.5631701631701632\n",
      "F1 Score: 0.6503364737550471\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the test data using the fitted scaler\n",
    "X_test, y_test = preprocess_fraud_test('fraudTest.csv', scaler)\n",
    "X_test_sel = selector.transform(X_test)\n",
    "\n",
    "# Evaluate the best estimator on the test set\n",
    "y_test_pred = best_rf.predict(X_test_sel)\n",
    "print(\"Test Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_test_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_test_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64695821-c903-4734-92ae-09011719687e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: RandomForestClassifier(min_samples_leaf=4, min_samples_split=3, n_estimators=71,\n",
      "                       random_state=42)\n",
      "Loaded: StandardScaler()\n"
     ]
    }
   ],
   "source": [
    "# Save the best model and scaler to disk\n",
    "joblib.dump(best_rf, 'best_rf_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "best_rf = joblib.load('best_rf_model.pkl')\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "print('Loaded:', best_rf)\n",
    "print('Loaded:', scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9b8901-337d-4b51-918a-b32f60ba95f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
