{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31df9474-68a8-4bfe-b56d-15e213b80c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f32750f0-9378-4025-a802-e67e1109931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fraud_data(file_path):\n",
    "    \"\"\"\n",
    "    Preprocess the fraud dataset and split into training and validation sets.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Processed training and validation sets (X_train, X_val, y_train, y_val).\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert 'trans_date_trans_time' to datetime\n",
    "    df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "\n",
    "    # Calculate age using 'dob' and transaction year\n",
    "    df['transaction_year'] = df['trans_date_trans_time'].dt.year\n",
    "    df['year_of_birth'] = pd.to_datetime(df['dob']).dt.year\n",
    "    df['age'] = df['transaction_year'] - df['year_of_birth']\n",
    "    df.drop(columns=['dob', 'transaction_year', 'year_of_birth'], inplace=True)\n",
    "\n",
    "    # Drop irrelevant columns\n",
    "    irrelevant_columns = ['Unnamed: 0', 'cc_num', 'trans_num', 'street']\n",
    "    df_cleaned = df.drop(columns=irrelevant_columns)\n",
    "\n",
    "    # Haversine function to calculate distance\n",
    "    def haversine(lat1, lon1, lat2, lon2):\n",
    "        lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "        c = 2 * asin(sqrt(a))\n",
    "        r = 6371  # Radius of Earth in kilometers.\n",
    "        return c * r\n",
    "\n",
    "    # Calculate distance and add to the dataset\n",
    "    df_cleaned['distance'] = df_cleaned.apply(\n",
    "        lambda row: haversine(row['lat'], row['long'], row['merch_lat'], row['merch_long']), axis=1)\n",
    "\n",
    "    # Create bins for latitude and longitude\n",
    "    n_bins = 10\n",
    "    df_cleaned['lat_bucket'] = pd.cut(df_cleaned['lat'], bins=n_bins, labels=False)\n",
    "    df_cleaned['long_bucket'] = pd.cut(df_cleaned['long'], bins=n_bins, labels=False)\n",
    "    df_cleaned['merch_lat_bucket'] = pd.cut(df_cleaned['merch_lat'], bins=n_bins, labels=False)\n",
    "    df_cleaned['merch_long_bucket'] = pd.cut(df_cleaned['merch_long'], bins=n_bins, labels=False)\n",
    "\n",
    "    # Encode categorical columns\n",
    "    categorical_columns = ['merchant', 'category', 'gender', 'job']\n",
    "    label_encoders = {}\n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        df_cleaned[col] = le.fit_transform(df_cleaned[col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "    # Drop columns that are no longer needed\n",
    "    columns_to_drop = ['trans_date_trans_time', 'first', 'last', 'city', 'state', 'zip', 'lat', 'long', 'merch_lat',\n",
    "                       'merch_long']\n",
    "    df_cleaned = df_cleaned.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Separate features and target variable\n",
    "    X = df_cleaned.drop(columns=['is_fraud'])\n",
    "    y = df_cleaned['is_fraud']\n",
    "\n",
    "    # Normalize numerical columns\n",
    "    numerical_columns = ['amt', 'age', 'distance', 'lat_bucket', 'long_bucket', 'merch_lat_bucket', 'merch_long_bucket']\n",
    "    scaler = StandardScaler()\n",
    "    X[numerical_columns] = scaler.fit_transform(X[numerical_columns])\n",
    "\n",
    "    # Split into training and validation sets (fixed parameters)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_val, y_train, y_val, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ff8f26-471b-498c-a58b-6f0780079b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val, scaler = preprocess_fraud_data('fraudTrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8165594-6c4d-45bc-981e-e9783b0ffbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pat\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2027/2027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.9790 - loss: 1127733.7500 - val_accuracy: 0.9941 - val_loss: 0.1461\n",
      "Epoch 2/5\n",
      "\u001b[1m2027/2027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9937 - loss: 4598.8525 - val_accuracy: 0.9941 - val_loss: 0.0581\n",
      "Epoch 3/5\n",
      "\u001b[1m2027/2027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9940 - loss: 792.0883 - val_accuracy: 0.9941 - val_loss: 0.0404\n",
      "Epoch 4/5\n",
      "\u001b[1m2027/2027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9943 - loss: 265.6897 - val_accuracy: 0.9941 - val_loss: 0.0365\n",
      "Epoch 5/5\n",
      "\u001b[1m2027/2027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9942 - loss: 192.6324 - val_accuracy: 0.9941 - val_loss: 0.0360\n",
      "\u001b[1m8105/8105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 404us/step\n",
      "Accuracy:  0.9941\n",
      "Precision: 0.0000\n",
      "Recall:    0.0000\n",
      "F1 Score:  0.0000\n",
      "\n",
      "Detailed classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    257815\n",
      "           1       0.00      0.00      0.00      1520\n",
      "\n",
      "    accuracy                           0.99    259335\n",
      "   macro avg       0.50      0.50      0.50    259335\n",
      "weighted avg       0.99      0.99      0.99    259335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build a simple feed-forward neural network\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),  \n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),  \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']  \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=512\n",
    ")\n",
    "\n",
    "# Predict and evaluate on the validation set\n",
    "y_val_pred_proba = model.predict(X_val)\n",
    "y_val_pred = (y_val_pred_proba > 0.5).astype(int)\n",
    "\n",
    "accuracy  = accuracy_score(y_val, y_val_pred)\n",
    "precision = precision_score(y_val, y_val_pred, zero_division=0)\n",
    "recall    = recall_score(y_val, y_val_pred, zero_division=0)\n",
    "f1        = f1_score(y_val, y_val_pred, zero_division=0)\n",
    "\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "# Report\n",
    "print(\"\\nDetailed classification report:\")\n",
    "print(classification_report(y_val, y_val_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f28fc8-bfbf-4cdb-aa53-854e588ab621",
   "metadata": {},
   "source": [
    "A 0.00 recall in class 1 means all actual frauds were missed.\n",
    "\n",
    "Oversampling / Undersampling;\n",
    "Adjust the Threshold;\n",
    "Gather More Fraud Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e077c721-fa07-46b5-91f3-bdf77b7135e2",
   "metadata": {},
   "source": [
    "# random oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9e33355-1eca-4135-818d-38ca504ebc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "# define oversampling strategy\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "# fit and apply the transform\n",
    "X_over, y_over = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17751fb8-afb9-46cb-b2fa-b1abff6b5dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pat\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4029/4029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.4993 - loss: 3170104.0000 - val_accuracy: 0.9941 - val_loss: 0.6914\n",
      "Epoch 2/5\n",
      "\u001b[1m4029/4029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.4996 - loss: 399.2000 - val_accuracy: 0.9941 - val_loss: 0.6916\n",
      "Epoch 3/5\n",
      "\u001b[1m4029/4029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.5004 - loss: 42.1779 - val_accuracy: 0.0059 - val_loss: 0.6936\n",
      "Epoch 4/5\n",
      "\u001b[1m4029/4029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.4996 - loss: 16.9885 - val_accuracy: 0.0059 - val_loss: 0.6964\n",
      "Epoch 5/5\n",
      "\u001b[1m4029/4029\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.5000 - loss: 12.1071 - val_accuracy: 0.0059 - val_loss: 0.6941\n",
      "\u001b[1m8105/8105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 433us/step\n",
      "Accuracy:  0.0059\n",
      "Precision: 0.0059\n",
      "Recall:    1.0000\n",
      "F1 Score:  0.0117\n",
      "\n",
      "Detailed classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00    257815\n",
      "           1       0.01      1.00      0.01      1520\n",
      "\n",
      "    accuracy                           0.01    259335\n",
      "   macro avg       0.00      0.50      0.01    259335\n",
      "weighted avg       0.00      0.01      0.00    259335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build a simple feed-forward neural network\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_over.shape[1],)),\n",
    "    Dropout(0.2),  \n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),  \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']  \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_over,\n",
    "    y_over,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=512\n",
    ")\n",
    "\n",
    "# Predict and evaluate on the validation set\n",
    "y_val_pred_proba = model.predict(X_val)\n",
    "y_val_pred = (y_val_pred_proba > 0.5).astype(int)\n",
    "\n",
    "accuracy  = accuracy_score(y_val, y_val_pred)\n",
    "precision = precision_score(y_val, y_val_pred, zero_division=0)\n",
    "recall    = recall_score(y_val, y_val_pred, zero_division=0)\n",
    "f1        = f1_score(y_val, y_val_pred, zero_division=0)\n",
    "\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "# Report\n",
    "print(\"\\nDetailed classification report:\")\n",
    "print(classification_report(y_val, y_val_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40ab2f45-3cd3-455a-bcc2-59df54248701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pat\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - accuracy: 0.5226 - loss: 0.7146 - val_accuracy: 0.0061 - val_loss: 1.2915\n",
      "Epoch 2/50\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.5380 - loss: 0.6871 - val_accuracy: 0.0059 - val_loss: 12.5662\n",
      "Epoch 3/50\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.5391 - loss: 0.6867 - val_accuracy: 0.0059 - val_loss: 1.3595\n",
      "Epoch 4/50\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.5409 - loss: 0.6864 - val_accuracy: 0.3168 - val_loss: 1.5131\n",
      "Epoch 5/50\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.5411 - loss: 0.6861 - val_accuracy: 0.0059 - val_loss: 0.8728\n",
      "Epoch 6/50\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.5414 - loss: 0.6858 - val_accuracy: 0.3678 - val_loss: 0.6872\n",
      "Epoch 7/50\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.5417 - loss: 0.6853 - val_accuracy: 0.1135 - val_loss: 1.6590\n",
      "Epoch 8/50\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.5433 - loss: 0.6850 - val_accuracy: 0.1575 - val_loss: 0.7026\n",
      "Epoch 9/50\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.5431 - loss: 0.6848 - val_accuracy: 0.2713 - val_loss: 0.6847\n",
      "Epoch 10/50\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.5437 - loss: 0.6847 - val_accuracy: 0.2150 - val_loss: 0.7849\n",
      "Epoch 11/50\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.5440 - loss: 0.6845 - val_accuracy: 0.1379 - val_loss: 0.7870\n",
      "Epoch 12/50\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.5432 - loss: 0.6845 - val_accuracy: 0.6724 - val_loss: 0.6625\n",
      "Epoch 13/50\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.5441 - loss: 0.6844 - val_accuracy: 0.9588 - val_loss: 0.5825\n",
      "Epoch 14/50\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.5446 - loss: 0.6843 - val_accuracy: 0.7265 - val_loss: 0.6691\n",
      "Epoch 15/50\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.5448 - loss: 0.6842 - val_accuracy: 0.9886 - val_loss: 0.6367\n",
      "Epoch 16/50\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.5451 - loss: 0.6844 - val_accuracy: 0.2539 - val_loss: 0.6804\n",
      "Epoch 17/50\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.5457 - loss: 0.6842 - val_accuracy: 0.0059 - val_loss: 1.0781\n",
      "Epoch 18/50\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.5450 - loss: 0.6842 - val_accuracy: 0.0059 - val_loss: 1.3901\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Build a deeper feed-forward neural network\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_over.shape[1],)),\n",
    "    BatchNormalization(),  # Normalization to stabilize training\n",
    "    Dropout(0.3),  \n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),  \n",
    "\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(1, activation='sigmoid')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=5e-4),  # Reduce learning rate\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model with Early Stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_over,\n",
    "    y_over,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,  # Increased from 5\n",
    "    batch_size=256,  # Reduced batch size for better updates\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5fd08e58-247c-4179-ab5a-65a3f0b4c24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8105/8105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 491us/step\n",
      "Accuracy:  0.9588\n",
      "Precision: 0.0085\n",
      "Recall:    0.0520\n",
      "F1 Score:  0.0146\n",
      "AUC-ROC Score: 0.5318\n",
      "Precision-Recall AUC: 0.0069\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98    257815\n",
      "           1       0.01      0.05      0.01      1520\n",
      "\n",
      "    accuracy                           0.96    259335\n",
      "   macro avg       0.50      0.51      0.50    259335\n",
      "weighted avg       0.99      0.96      0.97    259335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict and evaluate on the validation set\n",
    "y_val_pred_proba = model.predict(X_val)\n",
    "y_val_pred = (y_val_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy  = accuracy_score(y_val, y_val_pred)\n",
    "precision = precision_score(y_val, y_val_pred, zero_division=0)\n",
    "recall    = recall_score(y_val, y_val_pred, zero_division=0)\n",
    "f1        = f1_score(y_val, y_val_pred, zero_division=0)\n",
    "auc_roc   = roc_auc_score(y_val, y_val_pred_proba)\n",
    "pr_auc    = average_precision_score(y_val, y_val_pred_proba)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"AUC-ROC Score: {auc_roc:.4f}\")\n",
    "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26b4a497-0388-4dbc-9ff6-d2673744eab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pat\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 32ms/step - accuracy: 0.8984 - loss: 0.2386 - val_accuracy: 0.9542 - val_loss: 0.0903\n",
      "Epoch 2/20\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 32ms/step - accuracy: 0.9627 - loss: 0.0896 - val_accuracy: 0.9552 - val_loss: 0.0839\n",
      "Epoch 3/20\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 32ms/step - accuracy: 0.9727 - loss: 0.0714 - val_accuracy: 0.9572 - val_loss: 0.0942\n",
      "Epoch 4/20\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 32ms/step - accuracy: 0.9804 - loss: 0.0562 - val_accuracy: 0.9750 - val_loss: 0.0683\n",
      "Epoch 5/20\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 33ms/step - accuracy: 0.9848 - loss: 0.0470 - val_accuracy: 0.9801 - val_loss: 0.0545\n",
      "Epoch 6/20\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 33ms/step - accuracy: 0.9880 - loss: 0.0395 - val_accuracy: 0.9845 - val_loss: 0.0468\n",
      "Epoch 7/20\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 33ms/step - accuracy: 0.9903 - loss: 0.0337 - val_accuracy: 0.9853 - val_loss: 0.0496\n",
      "Epoch 8/20\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 33ms/step - accuracy: 0.9916 - loss: 0.0301 - val_accuracy: 0.9879 - val_loss: 0.0427\n",
      "Epoch 9/20\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 33ms/step - accuracy: 0.9925 - loss: 0.0281 - val_accuracy: 0.9874 - val_loss: 0.0438\n",
      "Epoch 10/20\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 34ms/step - accuracy: 0.9933 - loss: 0.0253 - val_accuracy: 0.9905 - val_loss: 0.0371\n",
      "Epoch 11/20\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 34ms/step - accuracy: 0.9940 - loss: 0.0234 - val_accuracy: 0.9920 - val_loss: 0.0323\n",
      "Epoch 12/20\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 34ms/step - accuracy: 0.9946 - loss: 0.0211 - val_accuracy: 0.9905 - val_loss: 0.0352\n",
      "Epoch 13/20\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 34ms/step - accuracy: 0.9948 - loss: 0.0208 - val_accuracy: 0.9910 - val_loss: 0.0344\n",
      "Epoch 14/20\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 35ms/step - accuracy: 0.9954 - loss: 0.0189 - val_accuracy: 0.9923 - val_loss: 0.0337\n",
      "Epoch 15/20\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 35ms/step - accuracy: 0.9957 - loss: 0.0180 - val_accuracy: 0.9924 - val_loss: 0.0323\n",
      "Epoch 16/20\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 36ms/step - accuracy: 0.9959 - loss: 0.0171 - val_accuracy: 0.9924 - val_loss: 0.0321\n",
      "Epoch 17/20\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 36ms/step - accuracy: 0.9962 - loss: 0.0160 - val_accuracy: 0.9924 - val_loss: 0.0316\n",
      "Epoch 18/20\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 37ms/step - accuracy: 0.9964 - loss: 0.0155 - val_accuracy: 0.9926 - val_loss: 0.0312\n",
      "Epoch 19/20\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 38ms/step - accuracy: 0.9963 - loss: 0.0155 - val_accuracy: 0.9930 - val_loss: 0.0307\n",
      "Epoch 20/20\n",
      "\u001b[1m8058/8058\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 40ms/step - accuracy: 0.9967 - loss: 0.0142 - val_accuracy: 0.9931 - val_loss: 0.0308\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Embedding, Flatten\n",
    "\n",
    "# Reshape data for LSTM (if needed)\n",
    "X_train_lstm = X_over.values.reshape((X_over.shape[0], X_over.shape[1], 1))\n",
    "X_val_lstm = X_val.values.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "\n",
    "# Build LSTM Model\n",
    "model = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(X_over.shape[1], 1)),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_lstm, y_over,\n",
    "    validation_data=(X_val_lstm, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=256,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "800d659a-9166-416e-9432-0c28d9aab9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8105/8105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step\n",
      "🔹 LSTM Model Results:\n",
      "Accuracy:  0.9930\n",
      "Precision: 0.4463\n",
      "Recall:    0.8395\n",
      "F1 Score:  0.5828\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00    257815\n",
      "           1       0.45      0.84      0.58      1520\n",
      "\n",
      "    accuracy                           0.99    259335\n",
      "   macro avg       0.72      0.92      0.79    259335\n",
      "weighted avg       1.00      0.99      0.99    259335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict and evaluate on the validation set\n",
    "y_val_pred_proba = model.predict(X_val_lstm)\n",
    "y_val_pred = (y_val_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy  = accuracy_score(y_val, y_val_pred)\n",
    "precision = precision_score(y_val, y_val_pred, zero_division=0)\n",
    "recall    = recall_score(y_val, y_val_pred, zero_division=0)\n",
    "f1        = f1_score(y_val, y_val_pred, zero_division=0)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"🔹 LSTM Model Results:\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "91b3b8d4-6a09-49dc-b82b-c0aac2900fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"fraud_detection_lstm.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a0014900-6c10-4de5-9e25-a85b46ae59b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pat\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 12 variables whereas the saved optimizer has 22 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model in the new Keras format\n",
    "loaded_model = load_model(\"fraud_detection_lstm.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1023c610-de0d-4dd2-9fa8-8b1d1af065a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
